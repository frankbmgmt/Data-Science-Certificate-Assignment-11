---
title: "Assignment 11"
author: "Scott Stoltzman"
date: "6/26/2019"
output: html_document
---

```{r}
library('tidyverse')
library('caret')
library('modelr')
library('titanic')

options(na.action = na.warn)
```

# The Titanic Data

```{r}
# Load the data
data("titanic_train") # training
all_data = titanic_train %>% 
  as_tibble() %>%
  mutate(Survived = factor(Survived)) # this is binary yes/no 0/1
```

## Explain why we have training data separate from testing data:

< Answer Here >


## Determine what's worth analyzing

We are going to try and predict `Survived`

```{r}
head(all_data)
```



```{r}
all_data %>% summary()
```


```{r}
all_data %>% glimpse()
```

## What is the following function doing?
< Answer Here >
```{r}
colSums(all_data == '')
```


## How should we handle missing values?
Looking at `Embarked`
```{r}
all_data %>% group_by(Embarked) %>% count()
```

## For learning purposes impute the following values:
  - Missing `Embarked` values should be `S`
  - Missing and `NA` values for `Age` should be replaced by the median `Age` 
  - `NA` values for `Fare` should be replaced by the mean `Fare` 
< Answer with code below >
```{r}
all_data = all_data %>%
  mutate(Embarked = if_else(Embarked == '', 'S', Embarked),
         Age = if_else(Age == '' | is.na(Age), median(Age, na.rm = TRUE), Age),
         Fare = if_else(is.na(Fare), mean(Fare, na.rm = TRUE), Fare))
all_data %>% summary()
```

## Prepare data to train a model
Why should we drop `PassengerId, Ticket, Name, Cabin` from our data?
< Answer Here>
< Answer in code below>
```{r}
all_data = all_data %>%
  select(-Ticket, -Name, -Cabin, -PassengerId)
```


Separate the data into test and train, call the new data (which has the imputed values) `train_data` and `test_data` to distinguish them from the raw data. Take the first 80% of rows as train and the last 20% of rows as test.
< Answer in code Below >
```{r}
train_split = 0.8
max_train_rows = round(0.8 * nrow(all_data))
starting_test_row = max_train_rows + 1
train_data = all_data[1:max_train_rows,]
test_data = all_data[starting_test_row:nrow(all_data),]
```

## What is wrong with the method above?
<Answer here>

## Build and train a model
Build a logistic regression by using the `glm` function (the family parameter as `family = binomial(link = "logit")`). Show a `summary()` of your model.
< Answer with code below >
```{r}
mod = glm(Survived ~., family = binomial(link = 'logit'), data = train_data)
summary(mod)
```

### Is there anything interesting in your model?
< Answer Here >

On your `train_data` -- `add_predictions()` requires a `type` parameter. Use `type = 'response'` and create predictions.
<Answer with code below>
```{r}
train_data %>%
  add_predictions(model = mod, var = 'pred', type = 'response')
```

How do you interpret the predictions from your model?
<Answer Here>

Create predictions for your `test_data` using the same method as above. Call your new tibble `results`
```{r}
results = test_data %>%
  add_predictions(model = mod, var = 'pred', type = 'response')
results
```

With your new test_data Set a cutoff point at `>= 0.6` for `Survived` to be `1` to replace `pred`
```{r}
results = results %>%
  mutate(pred = factor(if_else(pred > 0.6, 1, 0)))
```


Use the library `caret` to create a confusion matrix `confusionMatrix()` and describe the output.
```{r}
confusionMatrix(data=results$pred, reference=results$Survived)
```


## Major issues

We did not look at the balance of `Survived` (0 or 1). Why is this important?  

< Answer Here >

We built one model based off of the `titanic_train` data. Why should we have fit more?

< Answer Here >